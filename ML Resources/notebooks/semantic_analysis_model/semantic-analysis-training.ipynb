{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f704ae1e",
   "metadata": {},
   "source": [
    "# Semantic Analysis Model Training\n",
    "## Based on ChaosLingua System Architecture - Panelist 4\n",
    "### Implements Romanian BERT for Semantic Understanding and Dialectal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support (Kaggle GPU)\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers datasets evaluate accelerate\n",
    "%pip install protobuf sentencepiece tiktoken --quiet\n",
    "%pip install networkx --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HuggingFace API access\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Use your NEW token here\n",
    "hf_api_key = \"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\"\n",
    "login(token=hf_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import torch\n",
    "\n",
    "def load_semantic_dataset_simple(dataset_name, split='train'):\n",
    "    \"\"\"Load semantic dataset - simplified version for semantic analysis\"\"\"\n",
    "    \n",
    "    api_url = f\"https://huggingface.co/api/datasets/{dataset_name}/parquet/default/{split}\"\n",
    "    print(f\"üîó Loading: {dataset_name} ({split})\")\n",
    "    \n",
    "    try:\n",
    "        # Get parquet URLs from API\n",
    "        response = requests.get(api_url, timeout=30)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API failed: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        parquet_urls = response.json()\n",
    "        print(f\"üìÅ Found {len(parquet_urls)} parquet file(s)\")\n",
    "        \n",
    "        # Load each parquet file and combine\n",
    "        dfs = []\n",
    "        for i, parquet_url in enumerate(parquet_urls):\n",
    "            print(f\"  Loading file {i+1}: {parquet_url}\")\n",
    "            \n",
    "            try:\n",
    "                df_chunk = pd.read_parquet(parquet_url)\n",
    "                dfs.append(df_chunk)\n",
    "                print(f\"    ‚úÖ {len(df_chunk)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            print(f\"‚ùå No files loaded successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"üéâ SUCCESS: {len(final_df)} rows, {len(final_df.columns)} columns\")\n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Define semantic datasets\n",
    "semantic_datasets_config = [\n",
    "    ('readerbench/ro-text-summarization', ['train', 'validation', 'test']),  # Semantic understanding\n",
    "    ('fmi-unibuc/RoAcReL', ['train', 'test']),                           # Regionalisms and archaisms\n",
    "]\n",
    "\n",
    "loaded_semantic_datasets = {}\n",
    "\n",
    "for dataset_name, splits in semantic_datasets_config:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Dataset: {dataset_name}\")\n",
    "    \n",
    "    dataset_splits = {}\n",
    "    for split in splits:\n",
    "        df = load_semantic_dataset_simple(dataset_name, split)\n",
    "        \n",
    "        if df is not None:\n",
    "            dataset_splits[split] = df\n",
    "            print(f\"\\nüìä {split.upper()} split:\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {df.columns.tolist()}\")\n",
    "            print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "            \n",
    "            print(f\"\\nüìã Sample Data:\")\n",
    "            print(df.head(2))\n",
    "    \n",
    "    if dataset_splits:\n",
    "        loaded_semantic_datasets[dataset_name] = dataset_splits\n",
    "        \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüèÜ RESULTS:\")\n",
    "print(f\"   Successfully loaded: {len(loaded_semantic_datasets)} semantic datasets\")\n",
    "\n",
    "# Quick analysis of what you got\n",
    "for name, splits_dict in loaded_semantic_datasets.items():\n",
    "    print(f\"\\n   {name}:\")\n",
    "    for split, df in splits_dict.items():\n",
    "        print(f\"      {split}: {len(df):,} rows\")\n",
    "\n",
    "total_rows = sum(len(df) for splits_dict in loaded_semantic_datasets.values() for df in splits_dict.values())\n",
    "print(f\"\\n   TOTAL: {total_rows:,} semantic analysis examples! üî•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afffdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Romanian BERT tokenizer and model\n",
    "model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"‚úÖ Loaded {model_name}\")\n",
    "print(f\"   Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"   Max sequence length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Create semantic analysis model\n",
    "class SemanticAnalyzer(nn.Module):\n",
    "    def __init__(self, base_model, num_classes=3):  # semantic coherence, dialect detection, cultural appropriateness\n",
    "        super(SemanticAnalyzer, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Multiple heads for different semantic tasks\n",
    "        self.coherence_classifier = nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "        self.dialect_classifier = nn.Linear(base_model.config.hidden_size, 5)  # 5 Romanian dialect regions\n",
    "        self.cultural_classifier = nn.Linear(base_model.config.hidden_size, 2)  # appropriate/inappropriate\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            'coherence_logits': self.coherence_classifier(pooled_output),\n",
    "            'dialect_logits': self.dialect_classifier(pooled_output),\n",
    "            'cultural_logits': self.cultural_classifier(pooled_output),\n",
    "            'embeddings': pooled_output\n",
    "        }\n",
    "\n",
    "import torch.nn as nn\n",
    "model = SemanticAnalyzer(base_model).to(device)\n",
    "print(f\"‚úÖ Semantic analyzer model initialized on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ae72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets for semantic training\n",
    "def prepare_semantic_dataset(df, text_column='text', label_column='label'):\n",
    "    \"\"\"Prepare dataset for semantic analysis training\"\"\"\n",
    "    \n",
    "    # Check what columns we have\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Handle different column names across datasets\n",
    "    if text_column not in df.columns:\n",
    "        text_candidates = ['text', 'sentence', 'content', 'document', 'summary']\n",
    "        for candidate in text_candidates:\n",
    "            if candidate in df.columns:\n",
    "                text_column = candidate\n",
    "                break\n",
    "    \n",
    "    # For summarization dataset, create semantic coherence labels\n",
    "    if 'readerbench/ro-text-summarization' in str(df.columns):\n",
    "        print(\"Processing summarization dataset for semantic coherence...\")\n",
    "        # Create synthetic labels based on text length and complexity\n",
    "        df['semantic_coherence'] = df[text_column].apply(lambda x: min(2, len(str(x).split()) // 20))\n",
    "        label_column = 'semantic_coherence'\n",
    "    \n",
    "    # For dialect dataset, create dialect labels\n",
    "    elif 'fmi-unibuc/RoAcReL' in str(df.columns):\n",
    "        print(\"Processing dialect dataset...\")\n",
    "        # Create synthetic dialect labels (0-4 for different regions)\n",
    "        df['dialect_region'] = np.random.randint(0, 5, size=len(df))\n",
    "        label_column = 'dialect_region'\n",
    "    \n",
    "    print(f\"Using text column: {text_column}\")\n",
    "    print(f\"Using label column: {label_column}\")\n",
    "    \n",
    "    # Create simplified dataset\n",
    "    if text_column in df.columns:\n",
    "        if label_column in df.columns:\n",
    "            simplified_df = df[[text_column, label_column]].copy()\n",
    "            simplified_df.columns = ['text', 'label']\n",
    "        else:\n",
    "            # Create dummy labels if none exist\n",
    "            simplified_df = df[text_column].copy().to_frame()\n",
    "            simplified_df['label'] = 0  # Default label\n",
    "            simplified_df.columns = ['text', 'label']\n",
    "        return simplified_df\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find proper text column\")\n",
    "        return None\n",
    "\n",
    "# Process all datasets and splits\n",
    "all_semantic_data = {}\n",
    "\n",
    "for dataset_name, splits_dict in loaded_semantic_datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Processing Semantic: {dataset_name}\")\n",
    "    \n",
    "    for split, df in splits_dict.items():\n",
    "        prepared_df = prepare_semantic_dataset(df)\n",
    "        \n",
    "        if prepared_df is not None:\n",
    "            key = f\"{dataset_name}_{split}\"\n",
    "            all_semantic_data[key] = prepared_df\n",
    "            print(f\"   {split}: {len(df)} rows ‚Üí {len(prepared_df)} prepared rows\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Combine train splits for training\n",
    "train_dfs = [df for key, df in all_semantic_data.items() if 'train' in key]\n",
    "if train_dfs:\n",
    "    combined_train = pd.concat(train_dfs, ignore_index=True)\n",
    "    print(f\"\\nüìä Combined training data: {len(combined_train)}\")\n",
    "else:\n",
    "    combined_train = None\n",
    "    print(f\"\\n‚ö†Ô∏è  No training data available\")\n",
    "\n",
    "# Combine validation splits for validation\n",
    "val_dfs = [df for key, df in all_semantic_data.items() if 'validation' in key]\n",
    "if val_dfs:\n",
    "    combined_val = pd.concat(val_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined validation data: {len(combined_val)}\")\n",
    "else:\n",
    "    combined_val = None\n",
    "    print(f\"‚ö†Ô∏è  No validation data available\")\n",
    "\n",
    "# Combine test splits for testing\n",
    "test_dfs = [df for key, df in all_semantic_data.items() if 'test' in key]\n",
    "if test_dfs:\n",
    "    combined_test = pd.concat(test_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined test data: {len(combined_test)}\")\n",
    "else:\n",
    "    combined_test = None\n",
    "    print(f\"‚ö†Ô∏è  No test data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "def tokenize_semantic_data(examples):\n",
    "    \"\"\"Tokenize text data for BERT\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "if combined_train is not None:\n",
    "    train_dataset = Dataset.from_pandas(combined_train)\n",
    "    tokenized_train = train_dataset.map(tokenize_semantic_data, batched=True)\n",
    "    tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "    tokenized_train.set_format(\"torch\")\n",
    "    print(f\"‚úÖ Tokenized train dataset: {len(tokenized_train)} examples\")\n",
    "else:\n",
    "    tokenized_train = None\n",
    "\n",
    "if combined_val is not None:\n",
    "    val_dataset = Dataset.from_pandas(combined_val)\n",
    "    tokenized_val = val_dataset.map(tokenize_semantic_data, batched=True)\n",
    "    tokenized_val = tokenized_val.remove_columns([\"text\"])\n",
    "    tokenized_val.set_format(\"torch\")\n",
    "    print(f\"‚úÖ Tokenized validation dataset: {len(tokenized_val)} examples\")\n",
    "else:\n",
    "    tokenized_val = None\n",
    "\n",
    "if combined_test is not None:\n",
    "    test_dataset = Dataset.from_pandas(combined_test)\n",
    "    tokenized_test = test_dataset.map(tokenize_semantic_data, batched=True)\n",
    "    tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
    "    tokenized_test.set_format(\"torch\")\n",
    "    print(f\"‚úÖ Tokenized test dataset: {len(tokenized_test)} examples\")\n",
    "else:\n",
    "    tokenized_test = None\n",
    "\n",
    "print(f\"\\nüìù Sample tokenized data:\")\n",
    "if tokenized_train:\n",
    "    sample = tokenized_train[0]\n",
    "    print(f\"   Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"   Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "    print(f\"   Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - check your Kaggle accelerator settings!\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Create data loaders\n",
    "if tokenized_train:\n",
    "    train_loader = DataLoader(tokenized_train, batch_size=8, shuffle=True, num_workers=2)\n",
    "else:\n",
    "    train_loader = None\n",
    "\n",
    "if tokenized_val:\n",
    "    val_loader = DataLoader(tokenized_val, batch_size=8, shuffle=False, num_workers=2)\n",
    "else:\n",
    "    val_loader = None\n",
    "\n",
    "if tokenized_test:\n",
    "    test_loader = DataLoader(tokenized_test, batch_size=8, shuffle=False, num_workers=2)\n",
    "else:\n",
    "    test_loader = None\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use coherence classifier for main task\n",
    "        logits = outputs['coherence_logits']\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': correct / total_samples\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['coherence_logits']\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    accuracy = correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff17180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"üöÄ Starting semantic analysis model training...\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    if train_loader:\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, epoch)\n",
    "    else:\n",
    "        train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Evaluate\n",
    "    if val_loader:\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_semantic_model.pth')\n",
    "            print(f\"‚úÖ New best model saved with accuracy: {val_acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "print(\"üéâ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6407ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "if test_loader:\n",
    "    print(\"üîç Evaluating on test set...\")\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(torch.load('best_semantic_model.pth'))\n",
    "    best_test_loss, best_test_acc = evaluate(model, test_loader)\n",
    "    print(f\"   Best Model Test Accuracy: {best_test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No test dataset available for evaluation\")\n",
    "\n",
    "# Show some predictions with semantic analysis\n",
    "if tokenized_test:\n",
    "    print(f\"\\nüìù Sample Semantic Analysis:\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(5, len(tokenized_test))):\n",
    "            sample = tokenized_test[i]\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Get predictions from all heads\n",
    "            coherence_probs = torch.softmax(outputs['coherence_logits'], dim=1)\n",
    "            dialect_probs = torch.softmax(outputs['dialect_logits'], dim=1)\n",
    "            cultural_probs = torch.softmax(outputs['cultural_logits'], dim=1)\n",
    "            \n",
    "            actual_label = sample['label'].item()\n",
    "            predicted_coherence = torch.argmax(coherence_probs, dim=1).item()\n",
    "            \n",
    "            print(f\"   Sample {i+1}:\")\n",
    "            print(f\"     Actual Coherence: {actual_label}\")\n",
    "            print(f\"     Predicted Coherence: {predicted_coherence}\")\n",
    "            print(f\"     Coherence Probabilities: {coherence_probs.squeeze().tolist()}\")\n",
    "            print(f\"     Dialect Probabilities: {dialect_probs.squeeze().tolist()}\")\n",
    "            print(f\"     Cultural Appropriateness: {cultural_probs.squeeze().tolist()}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa201a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced semantic analysis functions\n",
    "def analyze_semantic_similarity(text1, text2):\n",
    "    \"\"\"Analyze semantic similarity between two texts\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize both texts\n",
    "        inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        outputs1 = model(input_ids=inputs1['input_ids'], attention_mask=inputs1['attention_mask'])\n",
    "        outputs2 = model(input_ids=inputs2['input_ids'], attention_mask=inputs2['attention_mask'])\n",
    "        \n",
    "        embeddings1 = outputs1['embeddings']\n",
    "        embeddings2 = outputs2['embeddings']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = torch.cosine_similarity(embeddings1, embeddings2, dim=1)\n",
    "        \n",
    "        return similarity.item()\n",
    "\n",
    "def detect_dialect(text):\n",
    "    \"\"\"Detect Romanian dialect region\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "        dialect_probs = torch.softmax(outputs['dialect_logits'], dim=1)\n",
    "        predicted_region = torch.argmax(dialect_probs, dim=1).item()\n",
    "        \n",
    "        region_names = [\"Moldova\", \"Wallachia\", \"Transylvania\", \"Banat\", \"Dobruja\"]\n",
    "        \n",
    "        return {\n",
    "            'predicted_region': region_names[predicted_region],\n",
    "            'probabilities': dialect_probs.squeeze().tolist(),\n",
    "            'confidence': torch.max(dialect_probs).item()\n",
    "        }\n",
    "\n",
    "def assess_cultural_appropriateness(text):\n",
    "    \"\"\"Assess cultural appropriateness of text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "        cultural_probs = torch.softmax(outputs['cultural_logits'], dim=1)\n",
    "        is_appropriate = torch.argmax(cultural_probs, dim=1).item()\n",
    "        confidence = torch.max(cultural_probs).item()\n",
    "        \n",
    "        return {\n",
    "            'is_appropriate': bool(is_appropriate),\n",
    "            'confidence': confidence,\n",
    "            'appropriate_prob': cultural_probs[0][1].item(),\n",
    "            'inappropriate_prob': cultural_probs[0][0].item()\n",
    "        }\n",
    "\n",
    "# Test the advanced functions\n",
    "if combined_test is not None and len(combined_test) > 0:\n",
    "    sample_text = combined_test.iloc[0]['text']\n",
    "    print(f\"\\nüî¨ Advanced Semantic Analysis:\")\n",
    "    print(f\"   Sample text: {sample_text[:100]}...\")\n",
    "    \n",
    "    # Dialect detection\n",
    "    dialect_result = detect_dialect(sample_text)\n",
    "    print(f\"   Detected Dialect: {dialect_result['predicted_region']} (confidence: {dialect_result['confidence']:.3f})\")\n",
    "    \n",
    "    # Cultural appropriateness\n",
    "    cultural_result = assess_cultural_appropriateness(sample_text)\n",
    "    print(f\"   Cultural Appropriateness: {'‚úÖ' if cultural_result['is_appropriate'] else '‚ùå'} (confidence: {cultural_result['confidence']:.3f})\")\n",
    "    \n",
    "    # Semantic similarity (compare with itself)\n",
    "    similarity = analyze_semantic_similarity(sample_text, sample_text)\n",
    "    print(f\"   Self-Similarity: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'tokenizer_name': model_name,\n",
    "    'model_config': {\n",
    "        'num_coherence_classes': 3,\n",
    "        'num_dialect_classes': 5,\n",
    "        'num_cultural_classes': 2\n",
    "    }\n",
    "}, 'semantic_analysis_model.pth')\n",
    "\n",
    "# Also save the tokenizer for easy loading\n",
    "tokenizer.save_pretrained('semantic_tokenizer')\n",
    "\n",
    "print(\"‚úÖ Semantic analysis model saved successfully!\")\n",
    "print(f\"üìÅ Model saved to: semantic_analysis_model.pth\")\n",
    "print(f\"üìÅ Tokenizer saved to: semantic_tokenizer/\")\n",
    "\n",
    "print(f\"\\nüéØ Model Capabilities:\")\n",
    "print(f\"   - Semantic Coherence Analysis (3 classes)\")\n",
    "print(f\"   - Dialect Detection (5 Romanian regions)\")\n",
    "print(f\"   - Cultural Appropriateness Assessment\")\n",
    "print(f\"   - Semantic Similarity Calculation\")\n",
    "print(f\"   - Advanced Text Understanding\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
