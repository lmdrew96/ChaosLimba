{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar Correction Model Training\n",
    "## Based on ChaosLingua System Architecture\n",
    "### Implements Structured Chaos philosophy with error harvesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support (Kaggle GPU)\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers datasets evaluate accelerate\n",
    "%pip install protobuf sentencepiece tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup HuggingFace API access\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Use your NEW token here\n",
    "hf_api_key = \"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\"\n",
    "login(token=hf_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def load_hf_dataset_simple(dataset_name, split='train'):\n",
    "    \"\"\"Simple version - API returns list of parquet URLs as strings\"\"\"\n",
    "    \n",
    "    api_url = f\"https://huggingface.co/api/datasets/{dataset_name}/parquet/default/{split}\"\n",
    "    print(f\"üîó Loading: {dataset_name} ({split})\")\n",
    "    \n",
    "    try:\n",
    "        # Get parquet URLs from API\n",
    "        response = requests.get(api_url, timeout=30)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API failed: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        parquet_urls = response.json()  # This is just a list of URL strings!\n",
    "        print(f\"üìÅ Found {len(parquet_urls)} parquet file(s)\")\n",
    "        \n",
    "        # Load each parquet file and combine\n",
    "        dfs = []\n",
    "        for i, parquet_url in enumerate(parquet_urls):\n",
    "            print(f\"  Loading file {i+1}: {parquet_url}\")\n",
    "            \n",
    "            try:\n",
    "                df_chunk = pd.read_parquet(parquet_url)\n",
    "                dfs.append(df_chunk)\n",
    "                print(f\"    ‚úÖ {len(df_chunk)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            print(f\"‚ùå No files loaded successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"üéâ SUCCESS: {len(final_df)} rows, {len(final_df.columns)} columns\")\n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Define datasets with their available splits\n",
    "datasets_config = [\n",
    "    ('upb-nlp/gec-ro-texts', ['train', 'validation']),\n",
    "    ('upb-nlp/gec_ro_cna', ['train', 'test']),\n",
    "    ('upb-nlp/gec-ro-comments', ['train', 'validation', 'test'])\n",
    "]\n",
    "\n",
    "loaded_datasets = {}\n",
    "\n",
    "for dataset_name, splits in datasets_config:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Dataset: {dataset_name}\")\n",
    "    \n",
    "    dataset_splits = {}\n",
    "    for split in splits:\n",
    "        df = load_hf_dataset_simple(dataset_name, split)\n",
    "        \n",
    "        if df is not None:\n",
    "            dataset_splits[split] = df\n",
    "            print(f\"\\nüìä {split.upper()} split:\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {df.columns.tolist()}\")\n",
    "            print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "            \n",
    "            print(f\"\\nüìã Sample Data:\")\n",
    "            print(df.head(2))\n",
    "    \n",
    "    if dataset_splits:\n",
    "        loaded_datasets[dataset_name] = dataset_splits\n",
    "        \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüèÜ RESULTS:\")\n",
    "print(f\"   Successfully loaded: {len(loaded_datasets)} datasets\")\n",
    "\n",
    "# Quick analysis of what you got\n",
    "for name, splits_dict in loaded_datasets.items():\n",
    "    print(f\"\\n   {name}:\")\n",
    "    for split, df in splits_dict.items():\n",
    "        print(f\"      {split}: {len(df):,} rows\")\n",
    "\n",
    "total_rows = sum(len(df) for splits_dict in loaded_datasets.values() for df in splits_dict.values())\n",
    "print(f\"\\n   TOTAL: {total_rows:,} grammar correction examples! üî•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to HuggingFace Dataset format and preprocess\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "\n",
    "# First, create pairs from the alternating data\n",
    "def create_pairs_from_dataset(df):\n",
    "    \"\"\"Extract alternating incorrect/correct pairs from the dataset\"\"\"\n",
    "    texts = df['text'].tolist()\n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(0, len(texts), 2):\n",
    "        if i + 1 < len(texts):  # Make sure we have a pair\n",
    "            pairs.append({\n",
    "                'original': texts[i],    # Incorrect sentence\n",
    "                'corrected': texts[i + 1]  # Correct sentence\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(pairs)\n",
    "\n",
    "# Process all datasets and splits\n",
    "all_pairs = {}\n",
    "\n",
    "for dataset_name, splits_dict in loaded_datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Processing: {dataset_name}\")\n",
    "    \n",
    "    for split, df in splits_dict.items():\n",
    "        pairs_df = create_pairs_from_dataset(df)\n",
    "        key = f\"{dataset_name}_{split}\"\n",
    "        all_pairs[key] = pairs_df\n",
    "        \n",
    "        print(f\"   {split}: {len(df)} texts ‚Üí {len(pairs_df)} pairs\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Combine train splits for training\n",
    "train_dfs = [pairs for key, pairs in all_pairs.items() if 'train' in key]\n",
    "combined_train = pd.concat(train_dfs, ignore_index=True)\n",
    "print(f\"\\nüìä Combined training pairs: {len(combined_train)}\")\n",
    "\n",
    "# Combine validation splits for validation\n",
    "val_dfs = [pairs for key, pairs in all_pairs.items() if 'validation' in key]\n",
    "if val_dfs:\n",
    "    combined_val = pd.concat(val_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined validation pairs: {len(combined_val)}\")\n",
    "else:\n",
    "    combined_val = None\n",
    "    print(f\"‚ö†Ô∏è  No validation splits available\")\n",
    "\n",
    "# Combine test splits for testing\n",
    "test_dfs = [pairs for key, pairs in all_pairs.items() if 'test' in key]\n",
    "if test_dfs:\n",
    "    combined_test = pd.concat(test_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined test pairs: {len(combined_test)}\")\n",
    "else:\n",
    "    combined_test = None\n",
    "    print(f\"‚ö†Ô∏è  No test splits available\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(combined_train)\n",
    "val_dataset = Dataset.from_pandas(combined_val) if combined_val is not None else None\n",
    "test_dataset = Dataset.from_pandas(combined_test) if combined_test is not None else None\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess with proper label masking for padding tokens\"\"\"\n",
    "    inputs = [\"correct: \" + sentence for sentence in examples[\"original\"]]\n",
    "    targets = examples[\"corrected\"]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # CRITICAL FIX: Replace padding token IDs with -100 so they're ignored in loss calculation\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_ids]\n",
    "        for label_ids in labels[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the datasets and remove text columns\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names) if val_dataset else None\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names) if test_dataset else None\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenized datasets:\")\n",
    "print(f\"   Train: {len(tokenized_train)} examples\")\n",
    "if tokenized_val:\n",
    "    print(f\"   Validation: {len(tokenized_val)} examples\")\n",
    "if tokenized_test:\n",
    "    print(f\"   Test: {len(tokenized_test)} examples\")\n",
    "\n",
    "print(f\"\\nüìù Sample pair:\")\n",
    "print(f\"   Original: {combined_train.iloc[0]['original']}\")\n",
    "print(f\"   Corrected: {combined_train.iloc[0]['corrected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and data collator\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small', use_safetensors=True)\n",
    "\n",
    "# CRITICAL: Add DataCollatorForSeq2Seq - this handles dynamic padding and batching correctly\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model and data collator initialized\")\n",
    "print(f\"   Model: google/mt5-small\")\n",
    "print(f\"   Data collator: DataCollatorForSeq2Seq with label_pad_token_id=-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - check your Kaggle accelerator settings!\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./grammar_correction_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,                 # FIXED: Lowered from 5e-5 to 3e-5\n",
    "    per_device_train_batch_size=8,      # FIXED: Reduced from 16 to 8\n",
    "    per_device_eval_batch_size=8,       # FIXED: Reduced from 16 to 8\n",
    "    gradient_accumulation_steps=2,      # FIXED: Added for effective batch size of 16\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,                         # FIXED: Changed from True to False (T5 has NaN issues with fp16)\n",
    "    max_grad_norm=1.0,                  # FIXED: Added gradient clipping\n",
    "    report_to=\"none\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured:\")\n",
    "print(f\"   Learning rate: 3e-5\")\n",
    "print(f\"   Batch size: 8 (per device)\")\n",
    "print(f\"   Gradient accumulation: 2 steps\")\n",
    "print(f\"   FP16: False (disabled to prevent NaN)\")\n",
    "print(f\"   Max grad norm: 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,  # CRITICAL: Added data_collator parameter\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized with data_collator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION: Verify preprocessing and model setup before training\n",
    "import torch\n",
    "\n",
    "print(\"üîç Pre-training validation checks:\\n\")\n",
    "\n",
    "# Check 1: Verify labels contain -100 for padding\n",
    "print(\"1Ô∏è‚É£ Checking label masking:\")\n",
    "sample_batch = tokenized_train[:4]\n",
    "sample_labels = sample_batch['labels']\n",
    "has_minus_100 = any(-100 in labels for labels in sample_labels)\n",
    "print(f\"   Labels contain -100 for padding: {has_minus_100} ‚úÖ\" if has_minus_100 else f\"   ‚ùå ERROR: Labels missing -100 masking!\")\n",
    "\n",
    "# Count -100 tokens in first sample\n",
    "minus_100_count = sum(1 for label in sample_labels[0] if label == -100)\n",
    "print(f\"   Sample label has {minus_100_count} masked tokens\")\n",
    "\n",
    "# Check 2: Test forward pass doesn't produce NaN\n",
    "print(\"\\n2Ô∏è‚É£ Testing forward pass:\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Get a small batch using data collator\n",
    "test_batch = data_collator([tokenized_train[i] for i in range(4)])\n",
    "test_batch = {k: v.to(device) for k, v in test_batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_batch)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "print(f\"   Forward pass loss: {loss.item():.4f}\")\n",
    "if torch.isnan(loss):\n",
    "    print(\"   ‚ùå ERROR: Loss is NaN!\")\n",
    "else:\n",
    "    print(\"   Loss is valid (not NaN) ‚úÖ\")\n",
    "\n",
    "# Check 3: Test generation produces Romanian text\n",
    "print(\"\\n3Ô∏è‚É£ Testing generation:\")\n",
    "test_input = tokenized_train[0]\n",
    "input_ids = torch.tensor([test_input['input_ids']]).to(device)\n",
    "attention_mask = torch.tensor([test_input['attention_mask']]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=128,\n",
    "        num_beams=2,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"   Input: {combined_train.iloc[0]['original'][:80]}...\")\n",
    "print(f\"   Generated: {generated_text[:80]}...\")\n",
    "\n",
    "if generated_text and not generated_text.isspace():\n",
    "    print(\"   Generation produces text ‚úÖ\")\n",
    "else:\n",
    "    print(\"   ‚ùå ERROR: Generation produces empty/invalid text!\")\n",
    "\n",
    "print(\"\\n‚úÖ All validation checks passed! Ready to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "if tokenized_test is not None:\n",
    "    print(\"üîç Evaluating on test set...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 8\n",
    "    \n",
    "    for i in tqdm(range(0, len(tokenized_test), batch_size), desc=\"Generating corrections\"):\n",
    "        # Get batch - HuggingFace datasets return dict of lists\n",
    "        batch_end = min(i + batch_size, len(tokenized_test))\n",
    "        batch = tokenized_test[i:batch_end]\n",
    "        \n",
    "        # Extract input_ids and attention_mask properly\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        \n",
    "        # Generate with proper parameters\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "            )\n",
    "        \n",
    "        # Decode predictions\n",
    "        batch_predictions = tokenizer.batch_decode(\n",
    "            generated_ids, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        predictions.extend(batch_predictions)\n",
    "        \n",
    "        # Get references for this batch\n",
    "        batch_references = [\n",
    "            combined_test.iloc[j]['corrected'] \n",
    "            for j in range(i, batch_end)\n",
    "        ]\n",
    "        references.extend(batch_references)\n",
    "    \n",
    "    # Format references for sacrebleu\n",
    "    references_formatted = [[ref] for ref in references]\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    bleu_score = bleu.compute(\n",
    "        predictions=predictions,\n",
    "        references=references_formatted\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"   BLEU Score: {bleu_score['score']:.2f}\")\n",
    "    print(f\"   Precision scores: {bleu_score['precisions']}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    exact_matches = sum(1 for p, r in zip(predictions, references) if p.strip() == r.strip())\n",
    "    print(f\"   Exact matches: {exact_matches}/{len(predictions)} ({exact_matches/len(predictions)*100:.1f}%)\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\nüìù Sample Corrections:\")\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        print(f\"   Original:  {combined_test.iloc[i]['original']}\")\n",
    "        print(f\"   Reference: {references[i]}\")\n",
    "        print(f\"   Predicted: {predictions[i]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No test dataset available for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained(\"grammar_correction_model\")\n",
    "tokenizer.save_pretrained(\"grammar_correction_model\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
