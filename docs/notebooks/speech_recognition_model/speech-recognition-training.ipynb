{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444de440",
   "metadata": {},
   "source": [
    "# Speech Recognition Model Training\n",
    "## Based on ChaosLingua System Architecture - Panelist 1\n",
    "### Implements Fine-tuned Whisper for Romanian Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a571e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support (Kaggle GPU)\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers datasets evaluate accelerate\n",
    "%pip install librosa soundfile --quiet\n",
    "%pip install protobuf sentencepiece tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HuggingFace API access\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Use your NEW token here\n",
    "hf_api_key = \"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\"\n",
    "login(token=hf_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "def load_asr_dataset_simple(dataset_name, split='train'):\n",
    "    \"\"\"Load ASR dataset - simplified version for speech recognition\"\"\"\n",
    "    \n",
    "    api_url = f\"https://huggingface.co/api/datasets/{dataset_name}/parquet/default/{split}\"\n",
    "    print(f\"üîó Loading: {dataset_name} ({split})\")\n",
    "    \n",
    "    try:\n",
    "        # Get parquet URLs from API\n",
    "        response = requests.get(api_url, timeout=30)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API failed: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        parquet_urls = response.json()\n",
    "        print(f\"üìÅ Found {len(parquet_urls)} parquet file(s)\")\n",
    "        \n",
    "        # Load each parquet file and combine\n",
    "        dfs = []\n",
    "        for i, parquet_url in enumerate(parquet_urls):\n",
    "            print(f\"  Loading file {i+1}: {parquet_url}\")\n",
    "            \n",
    "            try:\n",
    "                df_chunk = pd.read_parquet(parquet_url)\n",
    "                dfs.append(df_chunk)\n",
    "                print(f\"    ‚úÖ {len(df_chunk)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            print(f\"‚ùå No files loaded successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"üéâ SUCCESS: {len(final_df)} rows, {len(final_df.columns)} columns\")\n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Define ASR datasets\n",
    "asr_datasets_config = [\n",
    "    ('espnet/yodas2', ['train', 'validation']),  # Primary ASR dataset\n",
    "    ('qmeeus/vp-er-10l', ['train', 'test'])       # Voice processing features\n",
    "]\n",
    "\n",
    "loaded_asr_datasets = {}\n",
    "\n",
    "for dataset_name, splits in asr_datasets_config:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Dataset: {dataset_name}\")\n",
    "    \n",
    "    dataset_splits = {}\n",
    "    for split in splits:\n",
    "        df = load_asr_dataset_simple(dataset_name, split)\n",
    "        \n",
    "        if df is not None:\n",
    "            dataset_splits[split] = df\n",
    "            print(f\"\\nüìä {split.upper()} split:\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {df.columns.tolist()}\")\n",
    "            print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "            \n",
    "            print(f\"\\nüìã Sample Data:\")\n",
    "            print(df.head(2))\n",
    "    \n",
    "    if dataset_splits:\n",
    "        loaded_asr_datasets[dataset_name] = dataset_splits\n",
    "        \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüèÜ RESULTS:\")\n",
    "print(f\"   Successfully loaded: {len(loaded_asr_datasets)} ASR datasets\")\n",
    "\n",
    "# Quick analysis of what you got\n",
    "for name, splits_dict in loaded_asr_datasets.items():\n",
    "    print(f\"\\n   {name}:\")\n",
    "    for split, df in splits_dict.items():\n",
    "        print(f\"      {split}: {len(df):,} rows\")\n",
    "\n",
    "total_rows = sum(len(df) for splits_dict in loaded_asr_datasets.values() for df in splits_dict.values())\n",
    "print(f\"\\n   TOTAL: {total_rows:,} speech recognition examples! üî•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to HuggingFace Dataset format and preprocess\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize Whisper processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"romanian\", task=\"transcribe\")\n",
    "\n",
    "# Process datasets for ASR training\n",
    "def prepare_asr_dataset(df, audio_column='audio', text_column='text'):\n",
    "    \"\"\"Prepare dataset for ASR training with proper audio handling\"\"\"\n",
    "    \n",
    "    # Check what columns we have\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Handle different column names across datasets\n",
    "    if audio_column not in df.columns:\n",
    "        # Try common audio column names\n",
    "        audio_candidates = ['path', 'file', 'audio_path', 'file_path']\n",
    "        for candidate in audio_candidates:\n",
    "            if candidate in df.columns:\n",
    "                audio_column = candidate\n",
    "                break\n",
    "    \n",
    "    if text_column not in df.columns:\n",
    "        # Try common text column names\n",
    "        text_candidates = ['sentence', 'transcript', 'transcription', 'label']\n",
    "        for candidate in text_candidates:\n",
    "            if candidate in df.columns:\n",
    "                text_column = candidate\n",
    "                break\n",
    "    \n",
    "    print(f\"Using audio column: {audio_column}\")\n",
    "    print(f\"Using text column: {text_column}\")\n",
    "    \n",
    "    # Create simplified dataset with just audio and text\n",
    "    if audio_column in df.columns and text_column in df.columns:\n",
    "        simplified_df = df[[audio_column, text_column]].copy()\n",
    "        simplified_df.columns = ['audio', 'text']\n",
    "        return simplified_df\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find proper audio/text columns\")\n",
    "        return None\n",
    "\n",
    "# Process all datasets and splits\n",
    "all_asr_data = {}\n",
    "\n",
    "for dataset_name, splits_dict in loaded_asr_datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Processing ASR: {dataset_name}\")\n",
    "    \n",
    "    for split, df in splits_dict.items():\n",
    "        prepared_df = prepare_asr_dataset(df)\n",
    "        \n",
    "        if prepared_df is not None:\n",
    "            key = f\"{dataset_name}_{split}\"\n",
    "            all_asr_data[key] = prepared_df\n",
    "            print(f\"   {split}: {len(df)} rows ‚Üí {len(prepared_df)} prepared rows\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Combine train splits for training\n",
    "train_dfs = [df for key, df in all_asr_data.items() if 'train' in key]\n",
    "if train_dfs:\n",
    "    combined_train = pd.concat(train_dfs, ignore_index=True)\n",
    "    print(f\"\\nüìä Combined training data: {len(combined_train)}\")\n",
    "else:\n",
    "    combined_train = None\n",
    "    print(f\"\\n‚ö†Ô∏è  No training data available\")\n",
    "\n",
    "# Combine validation splits for validation\n",
    "val_dfs = [df for key, df in all_asr_data.items() if 'validation' in key]\n",
    "if val_dfs:\n",
    "    combined_val = pd.concat(val_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined validation data: {len(combined_val)}\")\n",
    "else:\n",
    "    combined_val = None\n",
    "    print(f\"‚ö†Ô∏è  No validation data available\")\n",
    "\n",
    "# Combine test splits for testing\n",
    "test_dfs = [df for key, df in all_asr_data.items() if 'test' in key]\n",
    "if test_dfs:\n",
    "    combined_test = pd.concat(test_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined test data: {len(combined_test)}\")\n",
    "else:\n",
    "    combined_test = None\n",
    "    print(f\"‚ö†Ô∏è  No test data available\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "if combined_train is not None:\n",
    "    train_dataset = Dataset.from_pandas(combined_train)\n",
    "    # Cast audio column to Audio feature\n",
    "    train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "else:\n",
    "    train_dataset = None\n",
    "\n",
    "if combined_val is not None:\n",
    "    val_dataset = Dataset.from_pandas(combined_val)\n",
    "    val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "else:\n",
    "    val_dataset = None\n",
    "\n",
    "if combined_test is not None:\n",
    "    test_dataset = Dataset.from_pandas(combined_test)\n",
    "    test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "else:\n",
    "    test_dataset = None\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Prepare audio and text for Whisper training\"\"\"\n",
    "    # Load and resample audio\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # Process audio features\n",
    "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features[0]\n",
    "    \n",
    "    # Process text labels\n",
    "    labels = processor(text=batch[\"text\"], return_tensors=\"pt\").input_ids[0]\n",
    "    \n",
    "    return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n",
    "# Tokenize the datasets\n",
    "if train_dataset:\n",
    "    tokenized_train = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names)\n",
    "    print(f\"\\n‚úÖ Tokenized train dataset: {len(tokenized_train)} examples\")\n",
    "else:\n",
    "    tokenized_train = None\n",
    "\n",
    "if val_dataset:\n",
    "    tokenized_val = val_dataset.map(prepare_dataset, remove_columns=val_dataset.column_names)\n",
    "    print(f\"‚úÖ Tokenized validation dataset: {len(tokenized_val)} examples\")\n",
    "else:\n",
    "    tokenized_val = None\n",
    "\n",
    "if test_dataset:\n",
    "    tokenized_test = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names)\n",
    "    print(f\"‚úÖ Tokenized test dataset: {len(tokenized_test)} examples\")\n",
    "else:\n",
    "    tokenized_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Set generation config for Romanian\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - check your Kaggle accelerator settings!\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ac3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./speech_recognition_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,     # Whisper needs more memory\n",
    "    per_device_eval_batch_size=8,      \n",
    "    gradient_accumulation_steps=4,      \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3815c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using WER (Word Error Rate)\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "if tokenized_test is not None:\n",
    "    print(\"üîç Evaluating on test set...\")\n",
    "    results = trainer.predict(tokenized_test)\n",
    "    \n",
    "    # Decode predictions\n",
    "    predictions = processor.batch_decode(results.predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Get references from test dataset\n",
    "    references = combined_test[\"text\"].tolist()\n",
    "    \n",
    "    # Calculate WER\n",
    "    wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"   Word Error Rate (WER): {wer_score:.4f}\")\n",
    "    print(f\"   Accuracy: {(1 - wer_score) * 100:.2f}%\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\nüìù Sample Transcriptions:\")\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        print(f\"   Reference: {references[i]}\")\n",
    "        print(f\"   Predicted: {predictions[i]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No test dataset available for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained(\"speech_recognition_model\")\n",
    "processor.save_pretrained(\"speech_recognition_model\")\n",
    "print(\"‚úÖ Speech recognition model saved successfully!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
