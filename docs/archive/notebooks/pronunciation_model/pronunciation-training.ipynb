{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c309052e",
   "metadata": {},
   "source": [
    "# Pronunciation Model Training\n",
    "## Based on ChaosLingua System Architecture - Panelist 2\n",
    "### Implements Acoustic Analyzer for Romanian Phonological Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support (Kaggle GPU)\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers datasets evaluate accelerate\n",
    "%pip install librosa soundfile --quiet\n",
    "%pip install protobuf sentencepiece tiktoken --quiet\n",
    "%pip install phonemetransformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HuggingFace API access\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Use your NEW token here\n",
    "hf_api_key = \"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\"\n",
    "login(token=hf_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def load_pronunciation_dataset_simple(dataset_name, split='train'):\n",
    "    \"\"\"Load pronunciation dataset - simplified version for phonological analysis\"\"\"\n",
    "    \n",
    "    api_url = f\"https://huggingface.co/api/datasets/{dataset_name}/parquet/default/{split}\"\n",
    "    print(f\"üîó Loading: {dataset_name} ({split})\")\n",
    "    \n",
    "    try:\n",
    "        # Get parquet URLs from API\n",
    "        response = requests.get(api_url, timeout=30)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API failed: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        parquet_urls = response.json()\n",
    "        print(f\"üìÅ Found {len(parquet_urls)} parquet file(s)\")\n",
    "        \n",
    "        # Load each parquet file and combine\n",
    "        dfs = []\n",
    "        for i, parquet_url in enumerate(parquet_urls):\n",
    "            print(f\"  Loading file {i+1}: {parquet_url}\")\n",
    "            \n",
    "            try:\n",
    "                df_chunk = pd.read_parquet(parquet_url)\n",
    "                dfs.append(df_chunk)\n",
    "                print(f\"    ‚úÖ {len(df_chunk)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            print(f\"‚ùå No files loaded successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all chunks\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"üéâ SUCCESS: {len(final_df)} rows, {len(final_df.columns)} columns\")\n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Define pronunciation datasets\n",
    "pronunciation_datasets_config = [\n",
    "    ('phonemetransformers/IPA-CHILDES', ['train', 'validation']),  # Primary phonological dataset\n",
    "    ('espnet/yodas2', ['train', 'validation']),                    # Audio-phoneme alignment\n",
    "    ('qmeeus/vp-er-10l', ['train', 'test'])                       # Voice characteristics\n",
    "]\n",
    "\n",
    "loaded_pronunciation_datasets = {}\n",
    "\n",
    "for dataset_name, splits in pronunciation_datasets_config:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Dataset: {dataset_name}\")\n",
    "    \n",
    "    dataset_splits = {}\n",
    "    for split in splits:\n",
    "        df = load_pronunciation_dataset_simple(dataset_name, split)\n",
    "        \n",
    "        if df is not None:\n",
    "            dataset_splits[split] = df\n",
    "            print(f\"\\nüìä {split.upper()} split:\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {df.columns.tolist()}\")\n",
    "            print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "            \n",
    "            print(f\"\\nüìã Sample Data:\")\n",
    "            print(df.head(2))\n",
    "    \n",
    "    if dataset_splits:\n",
    "        loaded_pronunciation_datasets[dataset_name] = dataset_splits\n",
    "        \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüèÜ RESULTS:\")\n",
    "print(f\"   Successfully loaded: {len(loaded_pronunciation_datasets)} pronunciation datasets\")\n",
    "\n",
    "# Quick analysis of what you got\n",
    "for name, splits_dict in loaded_pronunciation_datasets.items():\n",
    "    print(f\"\\n   {name}:\")\n",
    "    for split, df in splits_dict.items():\n",
    "        print(f\"      {split}: {len(df):,} rows\")\n",
    "\n",
    "total_rows = sum(len(df) for splits_dict in loaded_pronunciation_datasets.values() for df in splits_dict.values())\n",
    "print(f\"\\n   TOTAL: {total_rows:,} pronunciation examples! üî•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Pronunciation Analysis Model\n",
    "class PronunciationAnalyzer(nn.Module):\n",
    "    def __init__(self, audio_dim=80, hidden_dim=256, num_phonemes=50):\n",
    "        super(PronunciationAnalyzer, self).__init__()\n",
    "        \n",
    "        # Audio feature extractor (CNN)\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Conv1d(audio_dim, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Phoneme classifier\n",
    "        self.phoneme_classifier = nn.Sequential(\n",
    "            nn.Linear(512, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_phonemes)\n",
    "        )\n",
    "        \n",
    "        # Pronunciation quality scorer\n",
    "        self.quality_scorer = nn.Sequential(\n",
    "            nn.Linear(512, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, audio_features):\n",
    "        # Audio encoding\n",
    "        encoded = self.audio_encoder(audio_features)\n",
    "        encoded = encoded.view(encoded.size(0), -1)\n",
    "        \n",
    "        # Phoneme classification\n",
    "        phoneme_logits = self.phoneme_classifier(encoded)\n",
    "        \n",
    "        # Quality scoring\n",
    "        quality_score = self.quality_scorer(encoded)\n",
    "        \n",
    "        return {\n",
    "            'phoneme_logits': phoneme_logits,\n",
    "            'quality_score': quality_score,\n",
    "            'encoded_features': encoded\n",
    "        }\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PronunciationAnalyzer().to(device)\n",
    "print(f\"‚úÖ Pronunciation model initialized on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80fecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio preprocessing utilities\n",
    "def extract_audio_features(audio_path, target_sr=16000):\n",
    "    \"\"\"Extract MFCC features from audio\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
    "        \n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=80)\n",
    "        \n",
    "        # Normalize\n",
    "        mfccs = (mfccs - mfccs.mean()) / (mfccs.std() + 1e-8)\n",
    "        \n",
    "        return mfccs.T  # Transpose for (time, features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_pronunciation_dataset(df, audio_column='audio', phoneme_column='phoneme'):\n",
    "    \"\"\"Prepare dataset for pronunciation training\"\"\"\n",
    "    \n",
    "    # Check what columns we have\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Handle different column names across datasets\n",
    "    if audio_column not in df.columns:\n",
    "        audio_candidates = ['path', 'file', 'audio_path', 'file_path']\n",
    "        for candidate in audio_candidates:\n",
    "            if candidate in df.columns:\n",
    "                audio_column = candidate\n",
    "                break\n",
    "    \n",
    "    if phoneme_column not in df.columns:\n",
    "        phoneme_candidates = ['phoneme', 'ipa', 'transcription', 'label']\n",
    "        for candidate in phoneme_candidates:\n",
    "            if candidate in df.columns:\n",
    "                phoneme_column = candidate\n",
    "                break\n",
    "    \n",
    "    print(f\"Using audio column: {audio_column}\")\n",
    "    print(f\"Using phoneme column: {phoneme_column}\")\n",
    "    \n",
    "    # Create simplified dataset\n",
    "    if audio_column in df.columns and phoneme_column in df.columns:\n",
    "        simplified_df = df[[audio_column, phoneme_column]].copy()\n",
    "        simplified_df.columns = ['audio', 'phoneme']\n",
    "        return simplified_df\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find proper audio/phoneme columns\")\n",
    "        return None\n",
    "\n",
    "# Process all datasets and splits\n",
    "all_pronunciation_data = {}\n",
    "\n",
    "for dataset_name, splits_dict in loaded_pronunciation_datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üì¶ Processing Pronunciation: {dataset_name}\")\n",
    "    \n",
    "    for split, df in splits_dict.items():\n",
    "        prepared_df = prepare_pronunciation_dataset(df)\n",
    "        \n",
    "        if prepared_df is not None:\n",
    "            key = f\"{dataset_name}_{split}\"\n",
    "            all_pronunciation_data[key] = prepared_df\n",
    "            print(f\"   {split}: {len(df)} rows ‚Üí {len(prepared_df)} prepared rows\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Combine train splits for training\n",
    "train_dfs = [df for key, df in all_pronunciation_data.items() if 'train' in key]\n",
    "if train_dfs:\n",
    "    combined_train = pd.concat(train_dfs, ignore_index=True)\n",
    "    print(f\"\\nüìä Combined training data: {len(combined_train)}\")\n",
    "else:\n",
    "    combined_train = None\n",
    "    print(f\"\\n‚ö†Ô∏è  No training data available\")\n",
    "\n",
    "# Combine validation splits for validation\n",
    "val_dfs = [df for key, df in all_pronunciation_data.items() if 'validation' in key]\n",
    "if val_dfs:\n",
    "    combined_val = pd.concat(val_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined validation data: {len(combined_val)}\")\n",
    "else:\n",
    "    combined_val = None\n",
    "    print(f\"‚ö†Ô∏è  No validation data available\")\n",
    "\n",
    "# Combine test splits for testing\n",
    "test_dfs = [df for key, df in all_pronunciation_data.items() if 'test' in key]\n",
    "if test_dfs:\n",
    "    combined_test = pd.concat(test_dfs, ignore_index=True)\n",
    "    print(f\"üìä Combined test data: {len(combined_test)}\")\n",
    "else:\n",
    "    combined_test = None\n",
    "    print(f\"‚ö†Ô∏è  No test data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PronunciationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        self.phoneme_to_idx = {}\n",
    "        self.idx_to_phoneme = {}\n",
    "        \n",
    "        # Create phoneme vocabulary\n",
    "        all_phonemes = []\n",
    "        for phonemes in self.data['phoneme'].astype(str):\n",
    "            all_phonemes.extend(list(phonemes))\n",
    "        \n",
    "        unique_phonemes = sorted(set(all_phonemes))\n",
    "        self.phoneme_to_idx = {phoneme: idx for idx, phoneme in enumerate(unique_phonemes)}\n",
    "        self.idx_to_phoneme = {idx: phoneme for phoneme, idx in self.phoneme_to_idx.items()}\n",
    "        \n",
    "        print(f\"Created phoneme vocabulary with {len(unique_phonemes)} unique phonemes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Extract audio features\n",
    "        audio_features = extract_audio_features(row['audio'])\n",
    "        if audio_features is None:\n",
    "            # Create dummy features if audio processing fails\n",
    "            audio_features = np.random.randn(100, 80)\n",
    "        \n",
    "        # Convert phoneme to indices\n",
    "        phoneme_str = str(row['phoneme'])\n",
    "        phoneme_indices = [self.phoneme_to_idx.get(p, 0) for p in phoneme_str]\n",
    "        \n",
    "        # Create target (first phoneme for simplicity)\n",
    "        target = phoneme_indices[0] if phoneme_indices else 0\n",
    "        \n",
    "        return {\n",
    "            'audio_features': torch.FloatTensor(audio_features),\n",
    "            'phoneme_target': torch.LongTensor([target]),\n",
    "            'phoneme_sequence': torch.LongTensor(phoneme_indices)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "if combined_train is not None:\n",
    "    train_dataset = PronunciationDataset(combined_train)\n",
    "    print(f\"‚úÖ Training dataset: {len(train_dataset)} examples\")\n",
    "else:\n",
    "    train_dataset = None\n",
    "\n",
    "if combined_val is not None:\n",
    "    val_dataset = PronunciationDataset(combined_val)\n",
    "    print(f\"‚úÖ Validation dataset: {len(val_dataset)} examples\")\n",
    "else:\n",
    "    val_dataset = None\n",
    "\n",
    "if combined_test is not None:\n",
    "    test_dataset = PronunciationDataset(combined_test)\n",
    "    print(f\"‚úÖ Test dataset: {len(test_dataset)} examples\")\n",
    "else:\n",
    "    test_dataset = None\n",
    "\n",
    "# Create data loaders\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "else:\n",
    "    train_loader = None\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "else:\n",
    "    val_loader = None\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "else:\n",
    "    test_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb446e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - check your Kaggle accelerator settings!\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Update model with correct number of phonemes\n",
    "if train_dataset:\n",
    "    num_phonemes = len(train_dataset.phoneme_to_idx)\n",
    "    model = PronunciationAnalyzer(num_phonemes=num_phonemes).to(device)\n",
    "    print(f\"‚úÖ Model updated with {num_phonemes} phoneme classes\")\n",
    "\n",
    "# Training setup\n",
    "criterion_phoneme = nn.CrossEntropyLoss()\n",
    "criterion_quality = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    phoneme_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch in pbar:\n",
    "        audio_features = batch['audio_features'].to(device)\n",
    "        phoneme_targets = batch['phoneme_target'].squeeze().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(audio_features.transpose(1, 2))  # (batch, features, time)\n",
    "        \n",
    "        # Calculate losses\n",
    "        phoneme_loss = criterion_phoneme(outputs['phoneme_logits'], phoneme_targets)\n",
    "        \n",
    "        # Dummy quality targets (since we don't have explicit quality labels)\n",
    "        quality_targets = torch.ones_like(outputs['quality_score']) * 0.8  # Assume decent quality\n",
    "        quality_loss = criterion_quality(outputs['quality_score'], quality_targets)\n",
    "        \n",
    "        total_loss_batch = phoneme_loss + 0.5 * quality_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += total_loss_batch.item()\n",
    "        _, predicted = torch.max(outputs['phoneme_logits'], 1)\n",
    "        phoneme_correct += (predicted == phoneme_targets).sum().item()\n",
    "        total_samples += phoneme_targets.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': total_loss_batch.item(),\n",
    "            'acc': phoneme_correct / total_samples\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = phoneme_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, eval_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    phoneme_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            audio_features = batch['audio_features'].to(device)\n",
    "            phoneme_targets = batch['phoneme_target'].squeeze().to(device)\n",
    "            \n",
    "            outputs = model(audio_features.transpose(1, 2))\n",
    "            \n",
    "            phoneme_loss = criterion_phoneme(outputs['phoneme_logits'], phoneme_targets)\n",
    "            quality_targets = torch.ones_like(outputs['quality_score']) * 0.8\n",
    "            quality_loss = criterion_quality(outputs['quality_score'], quality_targets)\n",
    "            \n",
    "            total_loss_batch = phoneme_loss + 0.5 * quality_loss\n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs['phoneme_logits'], 1)\n",
    "            phoneme_correct += (predicted == phoneme_targets).sum().item()\n",
    "            total_samples += phoneme_targets.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    accuracy = phoneme_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80585768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"üöÄ Starting pronunciation model training...\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Evaluate\n",
    "    if val_loader:\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_pronunciation_model.pth')\n",
    "            print(f\"‚úÖ New best model saved with accuracy: {val_acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "print(\"üéâ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ed22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "if test_loader:\n",
    "    print(\"üîç Evaluating on test set...\")\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(torch.load('best_pronunciation_model.pth'))\n",
    "    best_test_loss, best_test_acc = evaluate(model, test_loader)\n",
    "    print(f\"   Best Model Test Accuracy: {best_test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No test dataset available for evaluation\")\n",
    "\n",
    "# Show some predictions\n",
    "if test_dataset:\n",
    "    print(f\"\\nüìù Sample Predictions:\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(5, len(test_dataset))):\n",
    "            sample = test_dataset[i]\n",
    "            audio_features = sample['audio_features'].unsqueeze(0).to(device)\n",
    "            \n",
    "            outputs = model(audio_features.transpose(1, 2))\n",
    "            _, predicted = torch.max(outputs['phoneme_logits'], 1)\n",
    "            \n",
    "            predicted_phoneme = test_dataset.idx_to_phoneme[predicted.item()]\n",
    "            actual_phoneme = test_dataset.idx_to_phoneme[sample['phoneme_target'].item()]\n",
    "            quality_score = outputs['quality_score'].item()\n",
    "            \n",
    "            print(f\"   Sample {i+1}:\")\n",
    "            print(f\"     Actual: {actual_phoneme}\")\n",
    "            print(f\"     Predicted: {predicted_phoneme}\")\n",
    "            print(f\"     Quality Score: {quality_score:.3f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'phoneme_to_idx': train_dataset.phoneme_to_idx if train_dataset else {},\n",
    "    'idx_to_phoneme': train_dataset.idx_to_phoneme if train_dataset else {},\n",
    "    'model_config': {\n",
    "        'audio_dim': 80,\n",
    "        'hidden_dim': 256,\n",
    "        'num_phonemes': len(train_dataset.phoneme_to_idx) if train_dataset else 50\n",
    "    }\n",
    "}, 'pronunciation_model.pth')\n",
    "\n",
    "print(\"‚úÖ Pronunciation model saved successfully!\")\n",
    "print(f\"üìÅ Model saved to: pronunciation_model.pth\")\n",
    "print(f\"üéØ Phoneme vocabulary size: {len(train_dataset.phoneme_to_idx) if train_dataset else 'N/A'}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
